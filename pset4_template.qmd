---
title: "PSET4"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID): Shuping Wu, shupingw
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_CL_\*\* \*\*\_SHW_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_1_\*\* Late coins left after submission: \*\*\_1_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)
1. & 2. 
```{python}
import pandas as pd

# 1: Load the 2016 POS Data
pos2016 = pd.read_csv('data/Q4_2016.csv')
pos2017 = pd.read_csv('data/Q4_2017.csv')
pos2018 = pd.read_csv('data/Q4_2018.csv')
pos2019 = pd.read_csv('data/Q4_2019.csv')
pos2016['Year'] = '2016'
pos2017['Year'] = '2017'
pos2018['Year'] = '2018'
pos2019['Year'] = '2019'
# 2: Filter for Short-Term Hospitals
short_term_hospitals_6 = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1.0) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]
num_hospitals_6 = short_term_hospitals_6.shape[0]

print(f"Number of short-term hospitals reported in 2016 data: {num_hospitals_6}")

unique_hospitals = short_term_hospitals_6.drop_duplicates(subset='FAC_NAME')
print(f"Number of unique hospitals: {len(unique_hospitals)}")
```

For part I: Download and explore the Provider of Services (POS) file, we only need the variables including PRVDR_CTGRY_SBTYP_CD, PRVDR_CTGRY_CD, FAC_NAME, PRVDR_NUM. 

In 2016, the United States had 4,840 community hospitals. These are defined as non-federal, short-term general and specialty hospitals whose facilities and services are available to the public. The source is from AHA (American Hospital Association): https://www.aha.org/system/files/2018-05/2018-chartbook-table-2-1.pdf.

Why it differs: pos2016 from CMS (Centers for Medicare & Medicaid Services) might include facilities beyond the typical definition of short-term hospitals, such as specialty clinics or outpatient centers, leading to an inflated count. Also, the dataset is from 2016, it may not reflect hospital closures, mergers, or reclassifications that have occurred.

3. 
```{python}
short_term_hospitals_7 = pos2017[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1)]
num_hospitals_7 = short_term_hospitals_7.shape[0]

print(f"Number of short-term hospitals reported in 2017 data: {num_hospitals_7}")

short_term_hospitals_8 = pos2018[(pos2018['PRVDR_CTGRY_CD'] == 1) & (pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1)]
num_hospitals_8 = short_term_hospitals_8.shape[0]

print(f"Number of short-term hospitals reported in 2018 data: {num_hospitals_8}")

short_term_hospitals_9 = pos2019[(pos2019['PRVDR_CTGRY_CD'] == 1) & (pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1)]
num_hospitals_9 = short_term_hospitals_9.shape[0]

print(f"Number of short-term hospitals reported in 2019 data: {num_hospitals_9}")
```

4. 
```{python}
import altair as alt

years = ['2016', '2017', '2018', '2019']
dataframes = []

for year in years:
    df = eval(f'pos{year}')
    df['Year'] = year 
    dataframes.append(df)

all_data = pd.concat(dataframes)

short_term_hospitals = all_data[(all_data['PRVDR_CTGRY_CD'] == 1) & (all_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]

total_hospitals_per_year = short_term_hospitals['Year'].value_counts().reset_index()
total_hospitals_per_year.columns = ['Year', 'Total Hospitals']
chart_unique_hospitals = alt.Chart(total_hospitals_per_year).mark_bar().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Total Hospitals', title='Number of Total Hospitals',),
    color=alt.value('blue')
).properties(
    title='Number of Total Short-Term Hospitals by Year'
).transform_calculate(
    adjusted_value='datum["Total Hospitals"]-7000'
).encode(
    y=alt.Y('adjusted_value:Q', title='Number of Total Hospitals (Adjusted)', scale=alt.Scale(domain=[0, 350]), axis=alt.Axis(labelExpr='datum.value + 7000'))
)

chart_unique_hospitals.display()

unique_hospitals_per_year = short_term_hospitals.groupby('Year')['PRVDR_NUM'].nunique().reset_index()
unique_hospitals_per_year.columns = ['Year', 'Unique Hospitals']

chart_unique_hospitals = alt.Chart(unique_hospitals_per_year).mark_bar().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Unique Hospitals', title='Number of Unique Hospitals',),
    color=alt.value('skyblue')
).properties(
    title='Number of Unique Short-Term Hospitals by Year'
).transform_calculate(
    adjusted_value='datum["Unique Hospitals"]-7000'
).encode(
    y=alt.Y('adjusted_value:Q', title='Number of Unique Hospitals (Adjusted)', scale=alt.Scale(domain=[0, 350]), axis=alt.Axis(labelExpr='datum.value + 7000'))
)

chart_unique_hospitals.display()
```

The number of observations equals the number of unique hospitals each year, it implies that each hospital is represented by a single unique entry in each dataset, with no repeated entries per hospital within a given year, confirming a straightforward data structure. 

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
active_2016 = short_term_hospitals[short_term_hospitals['Year'] == '2016']
active_2016_hospitals = active_2016[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].drop_duplicates()

suspected_closures = []

for year, df in zip(['2017', '2018', '2019'], [pos2017, pos2018, pos2019]):
    year_hospitals = df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    closed_hospitals = active_2016_hospitals[
        ~active_2016_hospitals['PRVDR_NUM'].isin(year_hospitals[year_hospitals['PGM_TRMNTN_CD'] == 0]['PRVDR_NUM'])
    ]
    
    closed_hospitals = closed_hospitals.assign(Suspected_Closure_Year=year)
    suspected_closures.append(closed_hospitals)

suspected_closures_df = pd.concat(suspected_closures).drop_duplicates(['PRVDR_NUM'])

closures_count = suspected_closures_df.shape[0]
print(f"Number of suspected closures: {closures_count}")
print(suspected_closures_df[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']])
```

2. 

```{python}
sorted_closures_top10 = suspected_closures_df.sort_values('FAC_NAME').head(10)

print(sorted_closures_top10[['FAC_NAME', 'Suspected_Closure_Year']])
```

3. 
    a. Among the suspected closures, 995 hospitals fit this definition of potentially
        being a merger/acquisition.
    b. After correcting for this, there are 3001 hospitals we've got left.


```{python}
suspected_closures_df = pd.concat(suspected_closures).drop_duplicates(['PRVDR_NUM'])
suspected_closures_df
active_hospitals_zip_year = short_term_hospitals[short_term_hospitals['PGM_TRMNTN_CD'] == 0].groupby(['ZIP_CD', 'Year']).size().unstack(fill_value=0)

merger_candidates = []

for _, row in suspected_closures_df.iterrows():
    zip_code = row['ZIP_CD']
    closure_year = row['Suspected_Closure_Year']
    
    if closure_year in ['2016', '2017', '2018']:
        next_year = str(int(closure_year) + 1)
        if zip_code in active_hospitals_zip_year.index:
            if active_hospitals_zip_year.loc[zip_code, next_year] >= active_hospitals_zip_year.loc[zip_code, closure_year]:
                merger_candidates.append(row)

merger_candidates_df = pd.DataFrame(merger_candidates)

merger_count = merger_candidates_df.shape[0]
print(f"Number of potential mergers/acquisitions: {merger_count}")
print(merger_candidates_df[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']])
```
    c.

```{python}
corrected_closures_df = suspected_closures_df[~suspected_closures_df['PRVDR_NUM'].isin(merger_candidates_df['PRVDR_NUM'])]

corrected_closures_count = corrected_closures_df.shape[0]

sorted_corrected_closures_top10 = corrected_closures_df.sort_values('FAC_NAME').head(10)

print(f"Number of corrected hospital closures: {corrected_closures_count}")
print(sorted_corrected_closures_top10[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']])
```




## Download Census zip code shapefile (10 pt) 

1. 
    a. -rw-r--r-- 1 15535 197609 6.2M Oct 30 23:58 gz_2010_us_860_00_500k.dbf
    
    b. -rw-r--r-- 1 15535 197609  165 Oct 30 23:58 gz_2010_us_860_00_500k.prj
    
    c. -rw-r--r-- 1 15535 197609 799M Oct 30 23:58 gz_2010_us_860_00_500k.shp
    
    d. -rw-r--r-- 1 15535 197609 259K Oct 30 23:58 gz_2010_us_860_00_500k.shx
    
    e. -rw-r--r-- 1 15535 197609  16K Oct 30 23:58 gz_2010_us_860_00_500k.xml


a. .dbf (Attribute Data File): Contains attribute data for each shape (e.g., names, values, categories), stored in a tabular format. This file links each shape’s attributes, providing additional context to the geometries stored in the .shp file.
Size: 6.2 MB

b. .prj (Projection File): Stores the coordinate system and projection information. This file defines how the geometrical data is projected on the Earth’s surface, essential for accurate spatial analysis and alignment with other datasets.
Size: 165 bytes

c. .shp (Shape File): Contains the geometry of the features, such as points, lines, or polygons. This is the core file of the shapefile dataset and is necessary for drawing the shapes on a map.
Size: 799 MB

d. .shx (Shape Index File): Stores the positional index of the feature geometry to allow quick access to the geometrical data in the .shp file. It acts as a spatial index that accelerates data retrieval.
Size: 259 KB

e. .xml (Metadata File): Contains metadata about the shapefile, such as the data’s source, description, and other attributes. Metadata provides context for the dataset’s creation and use.
Size: 16 KB

2. 
```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# Load the shapefile
zip_shapefile = gpd.read_file('data/gz_2010_us_860_00_500k.shp')


```
```{python}
# texas_choropleth
```
```{python}
# Filter for Texas ZIP codes (first two digits between 75 and 79)
texas_zip_codes = zip_shapefile[zip_shapefile['ZCTA5'].str[:3].isin(['733']) | zip_shapefile['ZCTA5'].str[:2].isin(['75', '76', '77', '78', '79'])]

# Filter and count hospitals per ZIP code in Texas
texas_hospitals = short_term_hospitals_6[
    short_term_hospitals_6['ZIP_CD'].astype(str).str[:3].isin(['733']) | 
    short_term_hospitals_6['ZIP_CD'].astype(str).str[:2].isin(['75', '76', '77', '78', '79'])
]
hospitals_per_zip = texas_hospitals['ZIP_CD'].value_counts().reset_index()
hospitals_per_zip.columns = ['ZIP_CD', 'Hospital_Count']

# Merge hospital count with Texas ZIP shapefile
texas_zip_codes['ZIP_CD'] = texas_zip_codes['ZCTA5'].astype(int).astype(str).str.zfill(5)
hospitals_per_zip['ZIP_CD'] = hospitals_per_zip['ZIP_CD'].astype(int).astype(str).str.zfill(5)
texas_choropleth = texas_zip_codes.merge(hospitals_per_zip, on='ZIP_CD', how='left').fillna(0)
```
```{python}
texas_choropleth.plot(column="Hospital_Count", cmap='Reds', legend=True)

```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1.  Dimensions are:
```{python}
#| warning: false
zips_all_centroids = zip_shapefile.copy()
zips_all_centroids['geometry'] = zips_all_centroids.centroid

dimensions = zips_all_centroids.shape
columns_info = zips_all_centroids.columns

dimensions, columns_info
```
2. (texas_zip_count, texas_borderstates_zip_count)
```{python}
texas_prefixes = ['75', '76', '77', '78', '79', '733']
bordering_states_prefixes = texas_prefixes + ['73', '74', '70', '71', '87', '88']

zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:3].isin(['733']) | zips_all_centroids['ZCTA5'].str[:2].isin(['75', '76', '77', '78', '79'])]

zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:2].isin(bordering_states_prefixes)]

texas_zip_count = zips_texas_centroids['ZCTA5'].nunique()
texas_borderstates_zip_count = zips_texas_borderstates_centroids['ZCTA5'].nunique()

texas_zip_count, texas_borderstates_zip_count
```
3. Merge on ZIP_CD, inner merge.
```{python}
zips_texas_borderstates_centroids['ZIP_CD'] = zips_texas_borderstates_centroids['ZCTA5'].astype(int).astype(str).str.zfill(5)
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospitals_per_zip[hospitals_per_zip['Hospital_Count'] > 0], 
    on='ZIP_CD', 
    how='inner'
)
print(f"Number of ZIP codes with at least one hospital: {zips_withhospital_centroids.shape[0]}")
zips_withhospital_centroids
```


4. 
    a.
```{python}
#| warning: false
from shapely.geometry import Point
import time

# Select a subset of 10 ZIP codes for testing
zips_texas_centroids_subset = zips_texas_centroids.head(10)

# Start timer
start_time = time.time()

# Calculate distance to the nearest ZIP code with a hospital for each ZIP in the subset
zips_texas_centroids_subset['nearest_hospital_distance'] = zips_texas_centroids_subset.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)

# End timer and calculate duration
duration = time.time() - start_time
print(f"Time taken for 10 ZIP codes: {duration:.2f} seconds")
```
    b.
```{python}
#| warning: false
# Start timer
start_time_full = time.time()

# Calculate distance to the nearest ZIP code with a hospital for each ZIP in the full dataset
zips_texas_centroids['nearest_hospital_distance'] = zips_texas_centroids.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)

# End timer and calculate full duration
duration_full = time.time() - start_time_full
print(f"Time taken for full calculation: {duration_full:.2f} seconds")
```
    c.


5. 
    a. Warning says it is in a geographic CRS, specifically degree. 
    b. This quite makes sense
```{python}
zips_texas_centroids = zips_texas_centroids.to_crs(epsg=32614)
zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=32614)

# Calculate distance to the nearest ZIP code with a hospital for each ZIP in Texas
start_time = time.time()
zips_texas_centroids['nearest_hospital_distance_meters'] = zips_texas_centroids.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)
duration = time.time() - start_time
print(f"Time taken for full calculation: {duration:.2f} seconds")

# Convert distances from meters to miles
zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['nearest_hospital_distance_meters'] * 0.000621371
zips_texas_centroids_distance_mean = zips_texas_centroids['nearest_hospital_distance_miles'].mean()
print(f"The average distance is: {zips_texas_centroids_distance_mean:.2f} miles")
# Verify results
print(zips_texas_centroids.head())
```
    

    c.
```{python}
zips_texas_centroids.plot(column='nearest_hospital_distance_miles',  legend=True)
```

## Effects of closures on access in Texas (15 pts)

1. 
```{python}
suspected_closures_df['ZIP_CD'] = suspected_closures_df['ZIP_CD'].astype(int).astype(str).str.zfill(5)

closure_table = suspected_closures_df[suspected_closures_df['ZIP_CD'].str[:3].isin(['733']) | suspected_closures_df['ZIP_CD'].str[:2].isin(['75', '76', '77', '78', '79'])]
closure_table = closure_table['ZIP_CD'].value_counts().reset_index()
closure_table.columns = ['ZIP_CD', 'Total Closure']
closure_table
```

2. 
```{python}

closure_choropleth = texas_zip_codes.merge(closure_table, on='ZIP_CD', how='right')

```
```{python}
closure_choropleth.plot(column = 'Total Closure', legend = True)
```

3. 
```{python}
closure_choropleth = closure_choropleth.to_crs(epsg=32614)

closure_choropleth['buffer_10_miles'] = closure_choropleth.geometry.buffer(16093.4)

# Create a GeoDataFrame containing just the buffered areas
buffers = closure_choropleth[['ZIP_CD', 'buffer_10_miles']]
buffers = buffers.set_geometry('buffer_10_miles')

# Perform a spatial join between the Texas ZIP codes and the buffers to find indirectly affected ZIP codes
indirectly_affected_zips = gpd.sjoin(zips_texas_centroids, buffers, how='inner', predicate='intersects')

# Count unique indirectly affected ZIP codes
indirectly_affected_zip_count = indirectly_affected_zips['ZIP_CD'].nunique()

# Display results
print(f"Number of indirectly affected ZIP codes in Texas: {indirectly_affected_zip_count}")
```

4. 
```{python}
zips_texas_centroids['Category'] = 'Not Affected'

# Label directly affected ZIP codes
directly_affected_zips = closure_choropleth['ZIP_CD'].tolist()
zips_texas_centroids.loc[zips_texas_centroids['ZCTA5'].isin(directly_affected_zips), 'Category'] = 'Directly Affected'

# Label indirectly affected ZIP codes (within 10 miles, but not directly affected)
indirectly_affected_zips_only = indirectly_affected_zips[~indirectly_affected_zips['ZIP_CD'].isin(directly_affected_zips)]
indirectly_affected_zip_codes = indirectly_affected_zips_only['ZIP_CD'].tolist()
zips_texas_centroids.loc[zips_texas_centroids['ZCTA5'].isin(indirectly_affected_zip_codes), 'Category'] = 'Indirectly Affected'

```
```{python}
zips_texas_centroids.plot(
    column='Category',  
    legend=True
)
```

## Reflecting on the exercise (10 pts) 
1. The potential issue here is accurately identifying hospital closures and openings year-over-year, given that simply looking at changes in hospital names might lead to incorrect conclusions. Some hospitals may appear to have "closed" or "opened" due to name changes or administrative updates, rather than actual closures or openings.
To address this, the approach suggested is filtering hospitals by unique identifiers of address and zipcode. By verifying changes based on location data rather than names alone, we can more reliably confirm actual closures and openings. For instance, if two hospitals with specific addresses and zipcodes are missing in a given year, they are likely closed. Conversely, new addresses and zipcodes indicate genuine openings.

2. The two approaches provide a basic approximation of hospital access. There are limitations such as 
the 10-mile radius might not accurately capture true accessibility. Some rural areas may have fewer transportation options, 
making a 10-mile distance a significant barrier, while urban areas may have better transit or multiple hospitals nearby.
Also, we are measuring the data based on zip code areas, which in reality it is subject to change and is not all accurate 
w.r.t. boundary measurements. There are potential ways to improve this, such as adding GIS information of transportation 
times into the system, considering hospital deisity and population.